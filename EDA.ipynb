{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6104ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Credit Fraud Competition - Full EDA Script\n",
    "# - Train/Test: unlabeled\n",
    "# - Val: labeled (EDA/평가 가능, fit은 train만)\n",
    "# Outputs saved to ./eda_outputs/\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, ranksums, skew, kurtosis\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# -----------------------------\n",
    "# Global config\n",
    "# -----------------------------\n",
    "RANDOM_SEED = 42\n",
    "OUT_DIR = \"eda_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47377ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0) Utilities\n",
    "# =============================\n",
    "def save_df(df: pd.DataFrame, name: str):\n",
    "    path = os.path.join(OUT_DIR, f\"{name}.csv\")\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[SAVE] {path} ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
    "\n",
    "def save_fig(name: str):\n",
    "    path = os.path.join(OUT_DIR, f\"{name}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=180)\n",
    "    plt.close()\n",
    "    print(f\"[SAVE] {path}\")\n",
    "\n",
    "def is_finite_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return np.isfinite(df.to_numpy())\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    # Convert to numeric if needed (coerce errors to NaN)\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def summarize_basic(df: pd.DataFrame, cols: List[str], prefix: str) -> pd.DataFrame:\n",
    "    # Per-column summary including tail quantiles and shape metrics\n",
    "    arr = df[cols]\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": cols,\n",
    "        f\"{prefix}_count\": [arr[c].notna().sum() for c in cols],\n",
    "        f\"{prefix}_nan\": [arr[c].isna().sum() for c in cols],\n",
    "        f\"{prefix}_mean\": [arr[c].mean() for c in cols],\n",
    "        f\"{prefix}_std\": [arr[c].std() for c in cols],\n",
    "        f\"{prefix}_median\": [arr[c].median() for c in cols],\n",
    "        f\"{prefix}_min\": [arr[c].min() for c in cols],\n",
    "        f\"{prefix}_q01\": [arr[c].quantile(0.01) for c in cols],\n",
    "        f\"{prefix}_q05\": [arr[c].quantile(0.05) for c in cols],\n",
    "        f\"{prefix}_q25\": [arr[c].quantile(0.25) for c in cols],\n",
    "        f\"{prefix}_q75\": [arr[c].quantile(0.75) for c in cols],\n",
    "        f\"{prefix}_q95\": [arr[c].quantile(0.95) for c in cols],\n",
    "        f\"{prefix}_q99\": [arr[c].quantile(0.99) for c in cols],\n",
    "        f\"{prefix}_max\": [arr[c].max() for c in cols],\n",
    "        f\"{prefix}_skew\": [skew(arr[c].dropna().to_numpy()) if arr[c].dropna().shape[0] > 2 else np.nan for c in cols],\n",
    "        f\"{prefix}_kurtosis\": [kurtosis(arr[c].dropna().to_numpy(), fisher=True) if arr[c].dropna().shape[0] > 3 else np.nan for c in cols],\n",
    "        f\"{prefix}_var\": [arr[c].var() for c in cols],\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def iqr_bounds(s: pd.Series) -> Tuple[float, float]:\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return (q1 - 1.5 * iqr, q3 + 1.5 * iqr)\n",
    "\n",
    "def compute_psi(expected: np.ndarray, actual: np.ndarray, bins: int = 20) -> float:\n",
    "    \"\"\"\n",
    "    PSI(Population Stability Index)\n",
    "    - expected: reference distribution (e.g., train)\n",
    "    - actual: target distribution (e.g., test/val)\n",
    "    \"\"\"\n",
    "    expected = expected[np.isfinite(expected)]\n",
    "    actual = actual[np.isfinite(actual)]\n",
    "    if expected.size < 10 or actual.size < 10:\n",
    "        return np.nan\n",
    "\n",
    "    # Quantile-based bins on expected (robust)\n",
    "    quantiles = np.linspace(0, 1, bins + 1)\n",
    "    cuts = np.unique(np.quantile(expected, quantiles))\n",
    "    if cuts.size < 3:  # too few unique cut points\n",
    "        return np.nan\n",
    "\n",
    "    # histogram counts\n",
    "    e_counts, _ = np.histogram(expected, bins=cuts)\n",
    "    a_counts, _ = np.histogram(actual, bins=cuts)\n",
    "\n",
    "    e_pct = e_counts / max(e_counts.sum(), 1)\n",
    "    a_pct = a_counts / max(a_counts.sum(), 1)\n",
    "\n",
    "    # add epsilon to avoid div by 0\n",
    "    eps = 1e-8\n",
    "    e_pct = np.clip(e_pct, eps, None)\n",
    "    a_pct = np.clip(a_pct, eps, None)\n",
    "\n",
    "    psi = np.sum((a_pct - e_pct) * np.log(a_pct / e_pct))\n",
    "    return float(psi)\n",
    "\n",
    "def effect_size_cohens_d(x0: np.ndarray, x1: np.ndarray) -> float:\n",
    "    x0 = x0[np.isfinite(x0)]\n",
    "    x1 = x1[np.isfinite(x1)]\n",
    "    if x0.size < 2 or x1.size < 2:\n",
    "        return np.nan\n",
    "    m0, m1 = x0.mean(), x1.mean()\n",
    "    s0, s1 = x0.std(ddof=1), x1.std(ddof=1)\n",
    "    sp = np.sqrt(((x0.size - 1) * s0**2 + (x1.size - 1) * s1**2) / max((x0.size + x1.size - 2), 1))\n",
    "    if sp == 0:\n",
    "        return np.nan\n",
    "    return float((m1 - m0) / sp)\n",
    "\n",
    "def safe_auc(y_true: np.ndarray, scores: np.ndarray) -> float:\n",
    "    # AUC requires both classes\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(roc_auc_score(y_true, scores))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def plot_feature_hist_overlay(train: pd.Series, other: pd.Series, title: str, other_label: str):\n",
    "    plt.figure()\n",
    "    # density hist overlay\n",
    "    plt.hist(train.dropna().to_numpy(), bins=60, density=True, alpha=0.5, label=\"train\")\n",
    "    plt.hist(other.dropna().to_numpy(), bins=60, density=True, alpha=0.5, label=other_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "def plot_box_by_class(val_df: pd.DataFrame, feature: str):\n",
    "    plt.figure()\n",
    "    x0 = val_df.loc[val_df[\"Class\"] == 0, feature].dropna().to_numpy()\n",
    "    x1 = val_df.loc[val_df[\"Class\"] == 1, feature].dropna().to_numpy()\n",
    "    plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n",
    "    plt.title(f\"Boxplot by Class: {feature}\")\n",
    "\n",
    "def plot_scatter_pca(pca_train: np.ndarray, pca_val: np.ndarray, y_val: np.ndarray):\n",
    "    plt.figure()\n",
    "    plt.scatter(pca_train[:, 0], pca_train[:, 1], s=6, alpha=0.25, label=\"train\")\n",
    "    plt.scatter(pca_val[y_val == 0, 0], pca_val[y_val == 0, 1], s=10, alpha=0.7, label=\"val class=0\")\n",
    "    plt.scatter(pca_val[y_val == 1, 0], pca_val[y_val == 1, 1], s=25, alpha=0.9, label=\"val class=1\")\n",
    "    plt.title(\"PCA (fit=train) - Train vs Val (colored by Class)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbeec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (113842, 31) val: (28462, 32) test: (142503, 31) sub: (142503, 2)\n",
      "[SAVE] eda_outputs\\meta.json\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 1) Load data\n",
    "# =============================\n",
    "train_path = \"open/train.csv\"\n",
    "val_path = \"open/val.csv\"\n",
    "test_path = \"open/test.csv\"\n",
    "sub_path = \"open/sample_submission.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "val = pd.read_csv(val_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sub = pd.read_csv(sub_path)\n",
    "\n",
    "print(\"train:\", train.shape, \"val:\", val.shape, \"test:\", test.shape, \"sub:\", sub.shape)\n",
    "\n",
    "# Columns\n",
    "id_col = \"ID\"\n",
    "class_col = \"Class\"\n",
    "feature_cols = [c for c in train.columns if c != id_col]\n",
    "\n",
    "# Ensure expected columns\n",
    "assert id_col in train.columns and id_col in val.columns and id_col in test.columns, \"ID column missing\"\n",
    "assert all(c in val.columns for c in feature_cols), \"val missing some features\"\n",
    "assert all(c in test.columns for c in feature_cols), \"test missing some features\"\n",
    "assert class_col in val.columns, \"val missing Class\"\n",
    "\n",
    "# Ensure numeric\n",
    "train = ensure_numeric(train, feature_cols)\n",
    "val = ensure_numeric(val, feature_cols + [class_col])\n",
    "test = ensure_numeric(test, feature_cols)\n",
    "\n",
    "# Split\n",
    "X_train = train[feature_cols].copy()\n",
    "X_val = val[feature_cols].copy()\n",
    "y_val = val[class_col].to_numpy().astype(int)\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "# Save a basic meta json\n",
    "meta = {\n",
    "    \"train_shape\": train.shape,\n",
    "    \"val_shape\": val.shape,\n",
    "    \"test_shape\": test.shape,\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"feature_cols\": feature_cols\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[SAVE] {os.path.join(OUT_DIR, 'meta.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a597ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 2) Integrity / Quality\n",
      "[SAVE] eda_outputs\\id_uniqueness.csv (3 rows, 4 cols)\n",
      "[SAVE] eda_outputs\\id_overlap.csv (3 rows, 2 cols)\n",
      "[SAVE] eda_outputs\\nan_inf_counts.csv (30 rows, 7 cols)\n",
      "[SAVE] eda_outputs\\train_variance_sorted.csv (30 rows, 2 cols)\n",
      "[SAVE] eda_outputs\\train_variance_hist.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 2) Integrity / Quality EDA\n",
    "# =============================\n",
    "print(\"\\n[EDA] 2) Integrity / Quality\")\n",
    "\n",
    "# ID uniqueness\n",
    "def id_report(df: pd.DataFrame, name: str) -> Dict:\n",
    "    n = df.shape[0]\n",
    "    uniq = df[id_col].nunique()\n",
    "    dup = n - uniq\n",
    "    return {\"dataset\": name, \"rows\": n, \"unique_ids\": uniq, \"duplicate_ids\": dup}\n",
    "\n",
    "id_rep = pd.DataFrame([\n",
    "    id_report(train, \"train\"),\n",
    "    id_report(val, \"val\"),\n",
    "    id_report(test, \"test\"),\n",
    "])\n",
    "save_df(id_rep, \"id_uniqueness\")\n",
    "\n",
    "# ID overlap across splits\n",
    "train_ids = set(train[id_col].tolist())\n",
    "val_ids = set(val[id_col].tolist())\n",
    "test_ids = set(test[id_col].tolist())\n",
    "\n",
    "overlap_rep = pd.DataFrame([\n",
    "    {\"pair\": \"train∩val\", \"overlap\": len(train_ids & val_ids)},\n",
    "    {\"pair\": \"train∩test\", \"overlap\": len(train_ids & test_ids)},\n",
    "    {\"pair\": \"val∩test\", \"overlap\": len(val_ids & test_ids)},\n",
    "])\n",
    "save_df(overlap_rep, \"id_overlap\")\n",
    "\n",
    "# NaN/Inf counts\n",
    "def nan_inf_report(X: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    nan_cnt = X.isna().sum()\n",
    "    inf_cnt = pd.Series(np.isinf(X.to_numpy()).sum(axis=0), index=X.columns)\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        f\"{name}_nan\": nan_cnt.values,\n",
    "        f\"{name}_inf\": inf_cnt.values\n",
    "    })\n",
    "    return out\n",
    "\n",
    "nan_inf = nan_inf_report(X_train, \"train\") \\\n",
    "    .merge(nan_inf_report(X_val, \"val\"), on=\"feature\") \\\n",
    "    .merge(nan_inf_report(X_test, \"test\"), on=\"feature\")\n",
    "\n",
    "save_df(nan_inf, \"nan_inf_counts\")\n",
    "\n",
    "# Near-constant features (variance small)\n",
    "var_train = X_train.var()\n",
    "near_const = pd.DataFrame({\n",
    "    \"feature\": var_train.index,\n",
    "    \"train_var\": var_train.values\n",
    "}).sort_values(\"train_var\", ascending=True)\n",
    "\n",
    "save_df(near_const, \"train_variance_sorted\")\n",
    "\n",
    "# Quick plot of variance distribution\n",
    "plt.figure()\n",
    "plt.hist(near_const[\"train_var\"].to_numpy(), bins=60)\n",
    "plt.title(\"Train feature variance distribution\")\n",
    "save_fig(\"train_variance_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e459b75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 3) Univariate Summary\n",
      "[SAVE] eda_outputs\\summary_univariate_all.csv (30 rows, 49 cols)\n",
      "[SAVE] eda_outputs\\train_tail_rank.csv (30 rows, 7 cols)\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V28.png\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V23.png\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V29.png\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V8.png\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V21.png\n",
      "[SAVE] eda_outputs\\hist_overlay_train_test_V20.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 3) Univariate summary (train/val/test)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 3) Univariate Summary\")\n",
    "\n",
    "sum_train = summarize_basic(train, feature_cols, \"train\")\n",
    "sum_val = summarize_basic(val, feature_cols, \"val\")\n",
    "sum_test = summarize_basic(test, feature_cols, \"test\")\n",
    "\n",
    "summary_all = sum_train.merge(sum_val, on=\"feature\").merge(sum_test, on=\"feature\")\n",
    "save_df(summary_all, \"summary_univariate_all\")\n",
    "\n",
    "# Tail heaviness ranking (by kurtosis, abs skew) on train\n",
    "tail_rank = summary_all[[\"feature\", \"train_skew\", \"train_kurtosis\", \"train_q99\", \"train_max\", \"train_min\"]] \\\n",
    "    .assign(abs_skew=lambda d: d[\"train_skew\"].abs()) \\\n",
    "    .sort_values([\"train_kurtosis\", \"abs_skew\"], ascending=False)\n",
    "\n",
    "save_df(tail_rank, \"train_tail_rank\")\n",
    "\n",
    "# Plot example: top 6 heavy-tail features overlay train vs test\n",
    "top6 = tail_rank[\"feature\"].head(6).tolist()\n",
    "for c in top6:\n",
    "    plot_feature_hist_overlay(train[c], test[c], title=f\"Train vs Test distribution: {c}\", other_label=\"test\")\n",
    "    save_fig(f\"hist_overlay_train_test_{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e1b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 4) Drift Metrics\n",
      "[SAVE] eda_outputs\\drift_ranked.csv (30 rows, 7 cols)\n",
      "[SAVE] eda_outputs\\drift_top_hist_V3.png\n",
      "[SAVE] eda_outputs\\drift_top_hist_V28.png\n",
      "[SAVE] eda_outputs\\drift_top_hist_V14.png\n",
      "[SAVE] eda_outputs\\drift_top_hist_V6.png\n",
      "[SAVE] eda_outputs\\drift_top_hist_V23.png\n",
      "[SAVE] eda_outputs\\drift_top_hist_V17.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 4) Drift EDA (train vs val/test)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 4) Drift Metrics\")\n",
    "\n",
    "drift_rows = []\n",
    "for c in feature_cols:\n",
    "    a = X_train[c].to_numpy()\n",
    "    b_val = X_val[c].to_numpy()\n",
    "    b_test = X_test[c].to_numpy()\n",
    "\n",
    "    # KS & Wasserstein (finite only)\n",
    "    a_f = a[np.isfinite(a)]\n",
    "    v_f = b_val[np.isfinite(b_val)]\n",
    "    t_f = b_test[np.isfinite(b_test)]\n",
    "\n",
    "    if a_f.size > 20 and v_f.size > 20:\n",
    "        ks_v = ks_2samp(a_f, v_f).statistic\n",
    "        wd_v = wasserstein_distance(a_f, v_f)\n",
    "        psi_v = compute_psi(a_f, v_f, bins=20)\n",
    "    else:\n",
    "        ks_v = wd_v = psi_v = np.nan\n",
    "\n",
    "    if a_f.size > 20 and t_f.size > 20:\n",
    "        ks_t = ks_2samp(a_f, t_f).statistic\n",
    "        wd_t = wasserstein_distance(a_f, t_f)\n",
    "        psi_t = compute_psi(a_f, t_f, bins=20)\n",
    "    else:\n",
    "        ks_t = wd_t = psi_t = np.nan\n",
    "\n",
    "    drift_rows.append({\n",
    "        \"feature\": c,\n",
    "        \"KS_train_val\": ks_v, \"Wass_train_val\": wd_v, \"PSI_train_val\": psi_v,\n",
    "        \"KS_train_test\": ks_t, \"Wass_train_test\": wd_t, \"PSI_train_test\": psi_t\n",
    "    })\n",
    "\n",
    "drift = pd.DataFrame(drift_rows)\n",
    "\n",
    "# Rank drift: primarily PSI_train_test then KS_train_test\n",
    "drift_rank = drift.sort_values([\"PSI_train_test\", \"KS_train_test\"], ascending=False)\n",
    "save_df(drift_rank, \"drift_ranked\")\n",
    "\n",
    "# Plot top 6 drift features overlay\n",
    "top6_drift = drift_rank[\"feature\"].head(6).tolist()\n",
    "for c in top6_drift:\n",
    "    plot_feature_hist_overlay(train[c], test[c], title=f\"DRIFT TOP - Train vs Test: {c}\", other_label=\"test\")\n",
    "    save_fig(f\"drift_top_hist_{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9a79e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 5) Val Label-based Analysis\n",
      "[SAVE] eda_outputs\\val_class_distribution.csv (2 rows, 3 cols)\n",
      "[SAVE] eda_outputs\\val_feature_separation_ranked.csv (30 rows, 9 cols)\n",
      "[SAVE] eda_outputs\\box_by_class_V10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V10.png\n",
      "[SAVE] eda_outputs\\box_by_class_V14.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V14.png\n",
      "[SAVE] eda_outputs\\box_by_class_V11.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V11.png\n",
      "[SAVE] eda_outputs\\box_by_class_V4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V4.png\n",
      "[SAVE] eda_outputs\\box_by_class_V12.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V12.png\n",
      "[SAVE] eda_outputs\\box_by_class_V3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V3.png\n",
      "[SAVE] eda_outputs\\box_by_class_V9.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V9.png\n",
      "[SAVE] eda_outputs\\box_by_class_V2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xrtwin\\AppData\\Local\\Temp\\ipykernel_23116\\3769959870.py:121: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([x0, x1], labels=[\"Class=0\", \"Class=1\"], showfliers=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\val_hist_by_class_V2.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 5) Label-based EDA on Val (fraud signals)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 5) Val Label-based Analysis\")\n",
    "\n",
    "val_counts = pd.DataFrame([\n",
    "    {\"class\": 0, \"count\": int((y_val == 0).sum())},\n",
    "    {\"class\": 1, \"count\": int((y_val == 1).sum())},\n",
    "])\n",
    "val_counts[\"ratio\"] = val_counts[\"count\"] / val_counts[\"count\"].sum()\n",
    "save_df(val_counts, \"val_class_distribution\")\n",
    "\n",
    "# Feature separation stats: ranksum p-value, Cohen's d, median diff, AUC (single feature)\n",
    "sep_rows = []\n",
    "for c in feature_cols:\n",
    "    x0 = X_val.loc[val[class_col] == 0, c].to_numpy()\n",
    "    x1 = X_val.loc[val[class_col] == 1, c].to_numpy()\n",
    "\n",
    "    x0f = x0[np.isfinite(x0)]\n",
    "    x1f = x1[np.isfinite(x1)]\n",
    "\n",
    "    if x0f.size > 5 and x1f.size > 2:\n",
    "        p = ranksums(x1f, x0f).pvalue  # (fraud vs normal)\n",
    "        d = effect_size_cohens_d(x0f, x1f)\n",
    "        med_diff = float(np.median(x1f) - np.median(x0f))\n",
    "        # AUC uses raw feature as score\n",
    "        auc = safe_auc(y_val, X_val[c].to_numpy())\n",
    "        # \"fraud outside normal IQR?\" indicators\n",
    "        lb, ub = iqr_bounds(pd.Series(x0f))\n",
    "        fraud_med = float(np.median(x1f))\n",
    "        outside_iqr = int(not (lb < fraud_med < ub))\n",
    "    else:\n",
    "        p = d = med_diff = auc = np.nan\n",
    "        outside_iqr = 0\n",
    "\n",
    "    sep_rows.append({\n",
    "        \"feature\": c,\n",
    "        \"ranksum_p\": p,\n",
    "        \"cohens_d(fraud-normal)\": d,\n",
    "        \"median_diff(fraud-normal)\": med_diff,\n",
    "        \"auc_single_feature\": auc,\n",
    "        \"fraud_median_outside_normal_IQR\": outside_iqr,\n",
    "        \"val_normal_n\": int(x0f.size),\n",
    "        \"val_fraud_n\": int(x1f.size),\n",
    "    })\n",
    "\n",
    "sep = pd.DataFrame(sep_rows)\n",
    "\n",
    "# rank: smallest p (strong diff), then |d| (effect size)\n",
    "sep[\"abs_d\"] = sep[\"cohens_d(fraud-normal)\"].abs()\n",
    "sep_rank = sep.sort_values([\"ranksum_p\", \"abs_d\"], ascending=[True, False])\n",
    "save_df(sep_rank, \"val_feature_separation_ranked\")\n",
    "\n",
    "# Plot top 8 separated features: boxplot by class + overlay hist\n",
    "top8_sep = sep_rank[\"feature\"].head(8).tolist()\n",
    "for c in top8_sep:\n",
    "    plot_box_by_class(val, c)\n",
    "    save_fig(f\"box_by_class_{c}\")\n",
    "\n",
    "    plt.figure()\n",
    "    x0 = X_val.loc[val[class_col] == 0, c].dropna().to_numpy()\n",
    "    x1 = X_val.loc[val[class_col] == 1, c].dropna().to_numpy()\n",
    "    plt.hist(x0, bins=60, density=True, alpha=0.55, label=\"Class=0\")\n",
    "    plt.hist(x1, bins=60, density=True, alpha=0.55, label=\"Class=1\")\n",
    "    plt.title(f\"Val distribution by Class: {c}\")\n",
    "    plt.legend()\n",
    "    save_fig(f\"val_hist_by_class_{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4436aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 6) Correlation & Redundancy\n",
      "[SAVE] eda_outputs\\top_corr_pairs_train.csv (200 rows, 4 cols)\n",
      "[SAVE] eda_outputs\\train_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 6) Correlation / Redundancy EDA (train)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 6) Correlation & Redundancy\")\n",
    "\n",
    "corr = X_train.corr(method=\"pearson\")\n",
    "# Save top correlated pairs\n",
    "corr_abs = corr.abs()\n",
    "np.fill_diagonal(corr_abs.values, 0.0)\n",
    "\n",
    "# extract top pairs\n",
    "pairs = []\n",
    "cols = corr_abs.columns.tolist()\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        pairs.append((cols[i], cols[j], corr.iloc[i, j], corr_abs.iloc[i, j]))\n",
    "top_corr = pd.DataFrame(pairs, columns=[\"f1\", \"f2\", \"corr\", \"abs_corr\"]).sort_values(\"abs_corr\", ascending=False)\n",
    "save_df(top_corr.head(200), \"top_corr_pairs_train\")\n",
    "\n",
    "# Heatmap-like visualization (matplotlib imshow)\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(corr.values, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Train Pearson Correlation (V1..V30)\")\n",
    "plt.xticks(range(len(feature_cols)), feature_cols, rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(feature_cols)), feature_cols, fontsize=6)\n",
    "save_fig(\"train_corr_heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7dae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 7) PCA Visualization (fit=train)\n",
      "[SAVE] eda_outputs\\pca_train_val_scatter.png\n",
      "[SAVE] eda_outputs\\pca_train_test_scatter.png\n",
      "[SAVE] eda_outputs\\pca_explained_variance.csv (1 rows, 2 cols)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 7) PCA Visualization (fit=train only)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 7) PCA Visualization (fit=train)\")\n",
    "\n",
    "pca = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "pca_train = pca.fit_transform(X_train.fillna(0).to_numpy())  # fit on train only\n",
    "pca_val = pca.transform(X_val.fillna(0).to_numpy())\n",
    "pca_test = pca.transform(X_test.fillna(0).to_numpy())\n",
    "\n",
    "plot_scatter_pca(pca_train, pca_val, y_val)\n",
    "save_fig(\"pca_train_val_scatter\")\n",
    "\n",
    "# Train vs Test PCA scatter (no labels)\n",
    "plt.figure()\n",
    "plt.scatter(pca_train[:, 0], pca_train[:, 1], s=6, alpha=0.25, label=\"train\")\n",
    "plt.scatter(pca_test[:, 0], pca_test[:, 1], s=6, alpha=0.25, label=\"test\")\n",
    "plt.title(\"PCA (fit=train) - Train vs Test\")\n",
    "plt.legend()\n",
    "save_fig(\"pca_train_test_scatter\")\n",
    "\n",
    "# Save PCA explained variance\n",
    "pca_info = pd.DataFrame([{\n",
    "    \"pca_explained_var_ratio_dim1\": float(pca.explained_variance_ratio_[0]),\n",
    "    \"pca_explained_var_ratio_dim2\": float(pca.explained_variance_ratio_[1]),\n",
    "}])\n",
    "save_df(pca_info, \"pca_explained_variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb761c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 8) IsolationForest Score Analysis (fit=train)\n",
      "[SAVE] eda_outputs\\val_fraud_ratio_reference.csv (1 rows, 1 cols)\n",
      "[SAVE] eda_outputs\\if_baseline_val_report.txt\n",
      "[SAVE] eda_outputs\\if_val_score_hist.png\n",
      "[SAVE] eda_outputs\\if_recall_at_k.csv (9 rows, 4 cols)\n",
      "[SAVE] eda_outputs\\if_contamination_sweep.csv (12 rows, 3 cols)\n",
      "[SAVE] eda_outputs\\if_contam_sweep_macro_f1.png\n",
      "[SAVE] eda_outputs\\if_contam_sweep_pred_ones.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 8) Anomaly-score EDA with IsolationForest (baseline-style)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 8) IsolationForest Score Analysis (fit=train)\")\n",
    "\n",
    "# initial contamination estimate from val label ratio (for reference)\n",
    "val_fraud_ratio = float((y_val == 1).mean())\n",
    "ref = pd.DataFrame([{\"val_fraud_ratio\": val_fraud_ratio}])\n",
    "save_df(ref, \"val_fraud_ratio_reference\")\n",
    "\n",
    "def if_predict_01(pred_1_minus1: np.ndarray) -> np.ndarray:\n",
    "    out = pred_1_minus1.copy()\n",
    "    out[out == 1] = 0\n",
    "    out[out == -1] = 1\n",
    "    return out\n",
    "\n",
    "# Fit IF on train only\n",
    "base_contam = max(min(val_fraud_ratio, 0.02), 0.0001)  # clamp for safety\n",
    "iso = IsolationForest(\n",
    "    n_estimators=1000,\n",
    "    contamination=base_contam,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "iso.fit(X_train.fillna(0).to_numpy())\n",
    "\n",
    "# scores: decision_function (higher=more normal), score_samples (higher=more normal in sklearn)\n",
    "val_pred = if_predict_01(iso.predict(X_val.fillna(0).to_numpy()))\n",
    "macro = f1_score(y_val, val_pred, average=\"macro\")\n",
    "cm = confusion_matrix(y_val, val_pred)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"if_baseline_val_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"contamination={base_contam}\\n\")\n",
    "    f.write(f\"macro_f1={macro}\\n\\n\")\n",
    "    f.write(\"confusion_matrix:\\n\")\n",
    "    f.write(str(cm) + \"\\n\\n\")\n",
    "    f.write(\"classification_report:\\n\")\n",
    "    f.write(classification_report(y_val, val_pred))\n",
    "print(f\"[SAVE] {os.path.join(OUT_DIR, 'if_baseline_val_report.txt')}\")\n",
    "\n",
    "# score distributions\n",
    "val_scores = iso.decision_function(X_val.fillna(0).to_numpy())  # higher = more normal\n",
    "plt.figure()\n",
    "plt.hist(val_scores[y_val == 0], bins=60, density=True, alpha=0.55, label=\"val class=0\")\n",
    "plt.hist(val_scores[y_val == 1], bins=60, density=True, alpha=0.55, label=\"val class=1\")\n",
    "plt.title(\"IsolationForest decision_function on Val (fit=train)\")\n",
    "plt.legend()\n",
    "save_fig(\"if_val_score_hist\")\n",
    "\n",
    "# top-k recall check (how many frauds in most anomalous k)\n",
    "# most anomalous => lowest decision_function\n",
    "order = np.argsort(val_scores)  # ascending => most anomalous first\n",
    "ks = [10, 20, 30, 50, 100, 200, 300, 500, 1000]\n",
    "rec_rows = []\n",
    "fraud_total = int((y_val == 1).sum())\n",
    "for k in ks:\n",
    "    k = min(k, len(y_val))\n",
    "    picked = y_val[order[:k]]\n",
    "    hit = int((picked == 1).sum())\n",
    "    rec_rows.append({\n",
    "        \"k\": k,\n",
    "        \"hits_fraud_in_topk\": hit,\n",
    "        \"fraud_total\": fraud_total,\n",
    "        \"recall_at_k\": hit / max(fraud_total, 1)\n",
    "    })\n",
    "rec_df = pd.DataFrame(rec_rows)\n",
    "save_df(rec_df, \"if_recall_at_k\")\n",
    "\n",
    "# contamination sweep (macro F1 curve)\n",
    "contams = np.unique(np.concatenate([\n",
    "    np.array([0.0005, 0.0008, 0.001, 0.0012, 0.0015, 0.002, 0.003, 0.005]),\n",
    "    np.linspace(0.0005, 0.005, 10)\n",
    "]))\n",
    "sweep_rows = []\n",
    "Xtr = X_train.fillna(0).to_numpy()\n",
    "Xva = X_val.fillna(0).to_numpy()\n",
    "for c in contams:\n",
    "    model = IsolationForest(n_estimators=600, contamination=float(c), random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    model.fit(Xtr)  # fit=train only\n",
    "    pred = if_predict_01(model.predict(Xva))\n",
    "    macro_f1 = f1_score(y_val, pred, average=\"macro\")\n",
    "    # count predicted fraud\n",
    "    n_pred1 = int((pred == 1).sum())\n",
    "    sweep_rows.append({\"contamination\": float(c), \"macro_f1\": float(macro_f1), \"predicted_ones\": n_pred1})\n",
    "\n",
    "sweep = pd.DataFrame(sweep_rows).sort_values(\"contamination\")\n",
    "save_df(sweep, \"if_contamination_sweep\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep[\"contamination\"], sweep[\"macro_f1\"], marker=\"o\")\n",
    "plt.title(\"Macro F1 vs contamination (IF, fit=train)\")\n",
    "plt.xlabel(\"contamination\")\n",
    "plt.ylabel(\"macro F1\")\n",
    "save_fig(\"if_contam_sweep_macro_f1\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sweep[\"contamination\"], sweep[\"predicted_ones\"], marker=\"o\")\n",
    "plt.title(\"Predicted #ones vs contamination (IF, fit=train)\")\n",
    "plt.xlabel(\"contamination\")\n",
    "plt.ylabel(\"# predicted Class=1\")\n",
    "save_fig(\"if_contam_sweep_pred_ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d0d9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 9) Sample-level FP/FN analysis (Val)\n",
      "[SAVE] eda_outputs\\val_fp_top50_by_anomaly_score.csv (15 rows, 34 cols)\n",
      "[SAVE] eda_outputs\\val_fn_top50_by_anomaly_score.csv (19 rows, 34 cols)\n",
      "[SAVE] eda_outputs\\val_tp_top50_by_anomaly_score.csv (11 rows, 34 cols)\n",
      "[SAVE] eda_outputs\\FP_top50_extreme_features_by_mean_abs_z.csv (30 rows, 2 cols)\n",
      "[SAVE] eda_outputs\\FP_top50_extreme_features_bar.png\n",
      "[SAVE] eda_outputs\\FN_top50_extreme_features_by_mean_abs_z.csv (30 rows, 2 cols)\n",
      "[SAVE] eda_outputs\\FN_top50_extreme_features_bar.png\n",
      "[SAVE] eda_outputs\\TP_top50_extreme_features_by_mean_abs_z.csv (30 rows, 2 cols)\n",
      "[SAVE] eda_outputs\\TP_top50_extreme_features_bar.png\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 9) Sample-level FP/FN Case Study (Val)\n",
    "# =============================\n",
    "print(\"\\n[EDA] 9) Sample-level FP/FN analysis (Val)\")\n",
    "\n",
    "val_df_case = val[[id_col] + feature_cols + [class_col]].copy()\n",
    "val_df_case[\"if_score\"] = val_scores\n",
    "val_df_case[\"pred\"] = val_pred\n",
    "\n",
    "fp = val_df_case[(val_df_case[class_col] == 0) & (val_df_case[\"pred\"] == 1)].copy()\n",
    "fn = val_df_case[(val_df_case[class_col] == 1) & (val_df_case[\"pred\"] == 0)].copy()\n",
    "tp = val_df_case[(val_df_case[class_col] == 1) & (val_df_case[\"pred\"] == 1)].copy()\n",
    "\n",
    "# Most anomalous among FP (lowest score)\n",
    "fp_top = fp.sort_values(\"if_score\").head(50)\n",
    "fn_top = fn.sort_values(\"if_score\").head(50)  # suspicious but missed\n",
    "tp_top = tp.sort_values(\"if_score\").head(50)\n",
    "\n",
    "save_df(fp_top[[id_col, class_col, \"pred\", \"if_score\"] + feature_cols], \"val_fp_top50_by_anomaly_score\")\n",
    "save_df(fn_top[[id_col, class_col, \"pred\", \"if_score\"] + feature_cols], \"val_fn_top50_by_anomaly_score\")\n",
    "save_df(tp_top[[id_col, class_col, \"pred\", \"if_score\"] + feature_cols], \"val_tp_top50_by_anomaly_score\")\n",
    "\n",
    "# Which features are extreme in FP compared to normal distribution?\n",
    "# Compute z-like using train mean/std (fit=train statistics only)\n",
    "mu = X_train.mean()\n",
    "sd = X_train.std().replace(0, np.nan)\n",
    "\n",
    "def extreme_feature_profile(samples: pd.DataFrame, name: str, topn: int = 12):\n",
    "    Xs = samples[feature_cols]\n",
    "    z = (Xs - mu) / sd\n",
    "    z_abs_mean = z.abs().mean().sort_values(ascending=False)\n",
    "    out = pd.DataFrame({\"feature\": z_abs_mean.index, f\"{name}_mean_abs_z\": z_abs_mean.values})\n",
    "    save_df(out.head(30), f\"{name}_extreme_features_by_mean_abs_z\")\n",
    "    # plot topn\n",
    "    top_features = z_abs_mean.head(topn).index.tolist()\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.bar(range(topn), z_abs_mean.head(topn).to_numpy())\n",
    "    plt.xticks(range(topn), top_features, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"{name}: Top extreme features (mean |z|, stats from train)\")\n",
    "    save_fig(f\"{name}_extreme_features_bar\")\n",
    "\n",
    "if len(fp) > 0:\n",
    "    extreme_feature_profile(fp_top, \"FP_top50\")\n",
    "if len(fn) > 0:\n",
    "    extreme_feature_profile(fn_top, \"FN_top50\")\n",
    "if len(tp) > 0:\n",
    "    extreme_feature_profile(tp_top, \"TP_top50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "303701e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 10) Optional nonlinear embedding (if available)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: random state is set to 42.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] eda_outputs\\pacmap_train_val_scatter.png\n",
      "[SKIP] UMAP not available or failed: ModuleNotFoundError(\"No module named 'umap'\")\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 10) Optional: Nonlinear embedding (PaCMAP/UMAP) if installed\n",
    "# =============================\n",
    "print(\"\\n[EDA] 10) Optional nonlinear embedding (if available)\")\n",
    "\n",
    "def try_pacmap():\n",
    "    try:\n",
    "        import pacmap\n",
    "        # Fit on train only, transform val\n",
    "        Xtr = X_train.fillna(0).to_numpy()\n",
    "        Xva = X_val.fillna(0).to_numpy()\n",
    "        emb = pacmap.PaCMAP(n_components=2, random_state=RANDOM_SEED, num_iters=400, verbose=False)\n",
    "        tr2 = emb.fit_transform(Xtr, init=\"pca\")\n",
    "        va2 = emb.transform(Xva, basis=Xtr)\n",
    "        plt.figure()\n",
    "        plt.scatter(tr2[:, 0], tr2[:, 1], s=6, alpha=0.25, label=\"train\")\n",
    "        plt.scatter(va2[y_val == 0, 0], va2[y_val == 0, 1], s=10, alpha=0.7, label=\"val class=0\")\n",
    "        plt.scatter(va2[y_val == 1, 0], va2[y_val == 1, 1], s=25, alpha=0.9, label=\"val class=1\")\n",
    "        plt.title(\"PaCMAP (fit=train) - Train vs Val (colored by Class)\")\n",
    "        plt.legend()\n",
    "        save_fig(\"pacmap_train_val_scatter\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[SKIP] PaCMAP not available or failed:\", repr(e))\n",
    "        return False\n",
    "\n",
    "def try_umap():\n",
    "    try:\n",
    "        import umap\n",
    "        Xtr = X_train.fillna(0).to_numpy()\n",
    "        Xva = X_val.fillna(0).to_numpy()\n",
    "        reducer = umap.UMAP(n_components=2, random_state=RANDOM_SEED)\n",
    "        tr2 = reducer.fit_transform(Xtr)      # fit=train only\n",
    "        va2 = reducer.transform(Xva)\n",
    "        plt.figure()\n",
    "        plt.scatter(tr2[:, 0], tr2[:, 1], s=6, alpha=0.25, label=\"train\")\n",
    "        plt.scatter(va2[y_val == 0, 0], va2[y_val == 0, 1], s=10, alpha=0.7, label=\"val class=0\")\n",
    "        plt.scatter(va2[y_val == 1, 0], va2[y_val == 1, 1], s=25, alpha=0.9, label=\"val class=1\")\n",
    "        plt.title(\"UMAP (fit=train) - Train vs Val (colored by Class)\")\n",
    "        plt.legend()\n",
    "        save_fig(\"umap_train_val_scatter\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[SKIP] UMAP not available or failed:\", repr(e))\n",
    "        return False\n",
    "\n",
    "_ = try_pacmap()\n",
    "_ = try_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53fb6dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EDA] 11) Write executive summary\n",
      "[SAVE] eda_outputs\\executive_summary.txt\n",
      "\n",
      "DONE. Check ./eda_outputs/ for CSV/PNG/TXT outputs.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 11) Final: Executive summary text\n",
    "# =============================\n",
    "print(\"\\n[EDA] 11) Write executive summary\")\n",
    "\n",
    "# top drift features\n",
    "top_drift = drift_rank.head(10)[[\"feature\", \"PSI_train_test\", \"KS_train_test\", \"Wass_train_test\"]]\n",
    "top_sep = sep_rank.head(10)[[\"feature\", \"ranksum_p\", \"abs_d\", \"auc_single_feature\", \"fraud_median_outside_normal_IQR\"]]\n",
    "\n",
    "summary_lines = []\n",
    "summary_lines.append(\"=== Executive Summary ===\")\n",
    "summary_lines.append(f\"- Train rows: {len(train)}, Val rows: {len(val)}, Test rows: {len(test)}\")\n",
    "summary_lines.append(f\"- #features: {len(feature_cols)}\")\n",
    "summary_lines.append(f\"- Val fraud ratio: {val_fraud_ratio:.6f} (used as reference for contamination)\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(\"[Top-10 Drift features: train vs test]\")\n",
    "summary_lines.append(top_drift.to_string(index=False))\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(\"[Top-10 Separation features on Val (label-based)]\")\n",
    "summary_lines.append(top_sep.to_string(index=False))\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(\"Next actions:\")\n",
    "summary_lines.append(\"- If drift is high, consider excluding those features or applying robust transforms (fit on train only).\")\n",
    "summary_lines.append(\"- Use separation ranking as 'hint' for feature selection, but avoid fitting transforms on val.\")\n",
    "summary_lines.append(\"- Analyze FP/FN case tables to identify features causing false positives and consider post-filtering rules.\")\n",
    "\n",
    "summary_path = os.path.join(OUT_DIR, \"executive_summary.txt\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "print(f\"[SAVE] {summary_path}\")\n",
    "\n",
    "print(\"\\nDONE. Check ./eda_outputs/ for CSV/PNG/TXT outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb08eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
